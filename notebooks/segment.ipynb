{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47d048a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DateType, LongType, BooleanType, FloatType, TimestampType\n",
    "from pyspark.sql import Row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b0f1508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/20 02:06:08 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "    .appName(\"example\")\n",
    "    .config(\"spark.sql.catalog.mycatalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    # .config(\"spark.sql.catalog.mycatalog.type\", \"rest\")\n",
    "    # .config(\"spark.sql.catalog.mycatalog.uri\", \"http://localhost\")\n",
    "    .config(\"spark.sql.catalog.mycatalog.type\", \"hive\")\n",
    "    .config(\"spark.sql.catalog.mycatalog.uri\", \"thrift://hive-metastore\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0241ad58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, name: string, test: boolean, employee: array<struct<id:bigint,name:string,contract:array<struct<id:bigint,contract_id:string,since:date,until:date,position:string>>>>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from demo.nyc.company\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0255475",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_schema = StructType([\n",
    "    StructField(\"id\", LongType(), nullable=False),\n",
    "    StructField(\"name\", StringType(), nullable=False),\n",
    "    StructField(\"test\", BooleanType(), nullable=False),\n",
    "    StructField(\"employee\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"id\", LongType(), nullable=False),\n",
    "            StructField(\"name\", StringType(), nullable=False),\n",
    "            StructField(\"contract\", ArrayType(\n",
    "                StructType([\n",
    "                    StructField(\"id\", LongType(), nullable=False),\n",
    "                    StructField(\"contract_id\", StringType(), nullable=False),\n",
    "                    StructField(\"since\", DateType(), nullable=False),\n",
    "                    StructField(\"until\", DateType(), nullable=True),\n",
    "                    StructField(\"position\", StringType(), nullable=True)\n",
    "                ])\n",
    "            ), nullable=True)\n",
    "        ])\n",
    "    ), nullable=True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27760dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "create table if not exists demo.nyc.company (\n",
    "    id bigint not null,\n",
    "    name string not null,\n",
    "    test boolean not null,\n",
    "    employee array<\n",
    "                struct<\n",
    "                    id bigint not null,\n",
    "                    name string not null,\n",
    "                    contract array<\n",
    "                        struct<\n",
    "                            id bigint not null,\n",
    "                            contract_id string not null,\n",
    "                            since date not null,\n",
    "                            until date,\n",
    "                            position string\n",
    "                        >\n",
    "                    >\n",
    "                >\n",
    "            >\n",
    ")\n",
    "using iceberg\n",
    "partitioned by (\n",
    "    id\n",
    ")\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e8eb230",
   "metadata": {},
   "outputs": [],
   "source": [
    "payslip_schema = StructType([\n",
    "    StructField(\"id\", LongType(), nullable=False),\n",
    "    StructField(\"type\", StringType(), nullable=False),\n",
    "    StructField(\"contract_id\", LongType(), nullable=False),\n",
    "    StructField(\"company_id\", LongType(), nullable=False),\n",
    "    StructField(\"period\", DateType(), nullable=False),\n",
    "    StructField(\"created_at\", DateType(), nullable=False),\n",
    "    StructField(\"employee_id\", LongType(), nullable=False),\n",
    "    StructField(\"item\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"id\", LongType(), nullable=False),\n",
    "            StructField(\"name\", StringType(), nullable=False),\n",
    "            StructField(\"value\", FloatType(), nullable=False)\n",
    "        ])\n",
    "    ), nullable=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66eb668e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_payslip = \"\"\"\n",
    "create table if not exists demo.nyc.payslip (\n",
    "    id bigint not null,\n",
    "    type string not null,\n",
    "    contract_id bigint not null,\n",
    "    company_id bigint not null,\n",
    "    period date not null,\n",
    "    created_at date not null,\n",
    "    employee_id bigint not null,\n",
    "    item array<\n",
    "            struct<\n",
    "                id bigint not null,\n",
    "                name string not null,\n",
    "                value float not null\n",
    "            >\n",
    "        >\n",
    ")\n",
    "using iceberg\n",
    "partitioned by (\n",
    "    company_id,\n",
    "    year(period)\n",
    ")\n",
    "\"\"\"\n",
    "spark.sql(sql_payslip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0eb3d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "workshift_schema = StructType([\n",
    "    StructField(\"id\", LongType(), nullable=False),\n",
    "    StructField(\"type\", StringType(), nullable=False),\n",
    "    StructField(\"company_id\", LongType(), nullable=False),\n",
    "    StructField(\"period\", DateType(), nullable=False),\n",
    "    StructField(\"since\", DateType(), nullable=False),\n",
    "    StructField(\"until\", DateType(), nullable=False),\n",
    "    StructField(\"employee_id\", LongType(), nullable=False),\n",
    "    StructField(\"mark\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"id\", LongType(), nullable=False),\n",
    "            StructField(\"type\", StringType(), nullable=False),\n",
    "            StructField(\"ts\", TimestampType(), nullable=False),\n",
    "            StructField(\"status\", StringType(), nullable=False)\n",
    "        ])\n",
    "    ), nullable=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9a2a162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_workshift = \"\"\"\n",
    "create table if not exists demo.nyc.workshift (\n",
    "    id bigint not null,\n",
    "    type string not null,\n",
    "    company_id bigint not null,\n",
    "    period date not null,\n",
    "    since date not null,\n",
    "    until date not null,\n",
    "    employee_id bigint not null,\n",
    "    mark array<\n",
    "            struct<\n",
    "                id bigint not null,\n",
    "                type string not null,\n",
    "                ts timestamp not null,\n",
    "                status string not null\n",
    "            >\n",
    "        >\n",
    ")\n",
    "\"\"\"\n",
    "spark.sql(sql_workshift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83b0adb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "import uuid\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "fake = Faker(['es_CL'])\n",
    "\n",
    "company_data = []\n",
    "employee_ids = []\n",
    "employee_id, contract_id = 0, 0\n",
    "for i in range(100):\n",
    "    data = {}\n",
    "    data[\"id\"] = i\n",
    "    data[\"name\"] = fake.company()\n",
    "    data[\"test\"] = i < 95\n",
    "    data[\"employee\"] = []\n",
    "    for ii in range(random.randint(0,500)):\n",
    "        contracts = []\n",
    "        for iii in range(random.randint(1,10)):\n",
    "            if iii == 0:\n",
    "                since = fake.date_between(start_date='-10y', end_date='-1w')\n",
    "            else:\n",
    "                since = until - relativedelta(days=1)\n",
    "            until = since + relativedelta(days=random.randint(30,500))\n",
    "            contracts.append({\n",
    "                \"id\": iii,\n",
    "                \"contract_id\": uuid.uuid4(),\n",
    "                \"since\": since,\n",
    "                \"until\": until,\n",
    "                \"position\": fake.job()\n",
    "            })\n",
    "            employee_ids.append((i,employee_id,contract_id))\n",
    "            contract_id += 1\n",
    "        data[\"employee\"].append({\n",
    "            \"id\": ii,\n",
    "            \"name\": fake.name(),\n",
    "            \"contract\": contracts\n",
    "        })\n",
    "        employee_id += 1\n",
    "    company_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "284b9626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# delete_sql = \"\"\"\n",
    "# truncate table demo.nyc.company\n",
    "# \"\"\"\n",
    "# spark.sql(delete_sql)\n",
    "company_df = spark.createDataFrame(company_data, company_schema)\n",
    "company_df.writeTo(\"demo.nyc.company\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf4e35d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "payslip_data = []\n",
    "item = 0\n",
    "payslip = 0\n",
    "for company, employee, contract in employee_ids:\n",
    "    items = []\n",
    "    for _ in range(random.randint(1,10)):\n",
    "        items.append({\n",
    "            \"id\": item,\n",
    "            \"name\": random.choice(['sueldo líquido', 'sueldo bruto', 'bono marzo', 'bono navidad', 'bono', 'salud', 'afp', 'seguro cesantía', 'adelanto', 'vacaciones']),\n",
    "            \"value\": float(random.randint(50000,1500000))\n",
    "        })\n",
    "        item += 1\n",
    "    payslip_data.append({\n",
    "        \"id\": payslip,\n",
    "        \"type\": random.choice(['sueldo', 'reproceso', 'finiquito']),\n",
    "        \"contract_id\": contract,\n",
    "        \"company_id\": company,\n",
    "        \"period\": fake.date_between(start_date='-1y', end_date='-1d'),\n",
    "        \"created_at\": fake.date_between(start_date='-1y', end_date='-1d'),\n",
    "        \"employee_id\": employee,\n",
    "        \"item\": items\n",
    "    })\n",
    "    payslip += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d43daa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "workshift_data = []\n",
    "workshift = 0\n",
    "mark = 0\n",
    "cmp, emp, cnt = zip(*employee_ids)\n",
    "employee_ids_wt = list(set(zip(cmp, emp)))\n",
    "for employee, company in employee_ids_wt:\n",
    "    marks = []\n",
    "    for _ in range(random.randint(2,60)):\n",
    "        marks.append({\n",
    "            \"id\": mark,\n",
    "            \"type\": random.choice(['zkteco', 'mobile', 'totem']),\n",
    "            \"ts\": fake.date_time_between(start_date=\"-1y\", end_date=\"-1d\"),\n",
    "            \"status\": random.choice(['ok', 'not ok'])\n",
    "        })\n",
    "        mark += 1\n",
    "    workshift_data.append({\n",
    "        \"id\": workshift,\n",
    "        \"type\": random.choice(['lunes a viernes', 'nocturno', 'rotativo']),\n",
    "        \"company_id\": company,\n",
    "        \"period\": fake.date_between(start_date=\"-1y\", end_date=\"-1d\"),\n",
    "        \"since\": fake.date_between(start_date=\"-1y\", end_date=\"-1d\"),\n",
    "        \"until\": fake.date_between(start_date=\"-1y\", end_date=\"-1d\"),\n",
    "        \"employee_id\": employee,\n",
    "        \"mark\": marks\n",
    "    })\n",
    "    workshift += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1c9d54b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/21 00:32:53 WARN TaskSetManager: Stage 9 contains a task of very large size (1092 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "payslip_df = spark.createDataFrame(payslip_data, payslip_schema)\n",
    "payslip_df.writeTo(\"demo.nyc.payslip\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dbe010f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/21 00:38:12 WARN TaskSetManager: Stage 12 contains a task of very large size (1283 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "workshift_df = spark.createDataFrame(workshift_data, workshift_schema)\n",
    "workshift_df.writeTo(\"demo.nyc.workshift\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fd8d315c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, type: string, company_id: bigint, period: date, since: date, until: date, employee_id: bigint, mark: array<struct<id:bigint,type:string,ts:timestamp,status:string>>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workshift_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba27bd93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
